# Functions to extract the Gini importance from trained random forests for the
# feature importance evaluation in Section 6.2 in the paper

# Michael Tschannen, ETH Zurich, 2016

import numpy as np
import pickle
import csv

from sklearn.ensemble import RandomForestRegressor


# Recursive function to create binary vectors for indexing the feature vector generated by the network
# according to depth, scales, and directions
def build_feature_unmap(depth, iprev, jprev, ndirs, nscales, imgsizes):
	
	depth_map = np.array([], dtype=int)
	scale_map = np.array([], dtype=int)
	dir_map = np.array([], dtype=int)
	
	# Mimic the flattening of the network architecture and record depths, scales, and directions
	
	# scale
	if iprev < nscales[depth]:
		for i in range(iprev,nscales[depth]):
			# direction
			for j in range(ndirs):
				depthn, scalen, dirn = build_feature_unmap(depth+1,i,j, ndirs, nscales, imgsizes)
				depth_map = np.append(depth_map, depthn)
				scale_map = np.append(scale_map, scalen)
				dir_map = np.append(dir_map, dirn)
	
	depth_map = np.append(depth_map, depth*np.ones(imgsizes[depth]))
	scale_map = np.append(scale_map, iprev*np.ones(imgsizes[depth]))
	dir_map = np.append(dir_map, jprev*np.ones(imgsizes[depth]))
	
	return (depth_map,scale_map,dir_map)



# Function to extract feature importance from trained random forest and stor them in a file
def unmap_feat_vec_csv(rf, fileprefix, ndirs, nscales, imgsizes, normalize=False):
	depth_map, scale_map, dir_map = build_feature_unmap(0, 0, 0, ndirs, nscales, imgsizes)
	
	barfile = open(fileprefix+'.dat','wb')
	barwriter = csv.writer(barfile, delimiter=' ', quoting=csv.QUOTE_NONE)
	barwriter.writerow(['header'])
	
	cscalefile = open(fileprefix+'_cumscales'+'.dat','wb')
	cscalewriter = csv.writer(cscalefile, delimiter=' ', quoting=csv.QUOTE_NONE)
	cscalewriter.writerow(['header'])
	
	clayerfile = open(fileprefix+'_cumlayers'+'.dat','wb')
	clayerwriter = csv.writer(clayerfile, delimiter=' ', quoting=csv.QUOTE_NONE)
	clayerwriter.writerow(['header'])
	
	feat_imps = np.zeros((rf.n_features_,len(rf.estimators_)))
	for i in range(feat_imps.shape[1]):
		feat_imps[:,i] = rf.estimators_[i].feature_importances_
	
	# depth
	for i in range(len(nscales)):
		cdepthind = (depth_map == i)
		cumlayer = 0
		nlayer = 0
		# direction
		for j in range(int(max(dir_map[cdepthind]))+1):
			cdirind = cdepthind & (dir_map == j)
			cumscale = 0
			nscale = 0
			# scale
			bar_data = []
			for k in range(int(max(scale_map[cdirind]))+1):
				cimp_trees = np.sum(feat_imps[cdirind & (scale_map == k),:],axis=0)
				cimp = np.mean(cimp_trees)
				cstd = np.std(cimp_trees)/np.sqrt(len(cimp_trees))
				nfeats = sum(cdirind & (scale_map == k))
				
				cumscale += cimp
				nscale += nfeats
				cumlayer += cimp
				nlayer += nfeats
				
				if normalize:
					cimp /= nfeats
				
				print(str([i,j,k])+' avg importance: '+str(cimp)+', std error: '+str(cstd))
				bar_data.append(str(cimp))
				bar_data.append(str(cstd))
				
			
			if len(bar_data) < max(nscales):
				bar_data.extend([0.0 for k in range(2*(int(max(nscales))-len(bar_data) + 1))])
			
			bar_data.insert(0,str(i)+'/'+str(j))
			barwriter.writerow(bar_data)
			
			if normalize:
				cumscale /= nscale
			
			cscalewriter.writerow([str(i)+'/'+str(j), str(cumscale)])
			
		if normalize:
			cumlayer /= nscale
		
		print('Layer '+str(i)+': '+str(cumlayer))
		clayerwriter.writerow([str(cumlayer)])
	
	barwriter.writerow(bar_data)
	barfile.close()
	
	cscalefile.close()
	clayerfile.close()
	

# pickle load utility
def load_pkl(fname):
	pfile = open(fname, 'rb')
	f = pickle.load(pfile)
	pfile.close()
	return f
